---
title: "Parallel Tempering"
author: "Bob Verity and Pete Winskill"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Parallel Tempering}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, echo = FALSE}
# set random seed
set.seed(1)

# load the drjacoby package
library(drjacoby)

# load other packages
library(ggplot2)
library(gridExtra)
```

MCMC becomes considerably harder when the posterior distribution is 1) highly correlated, and/or 2) highly multimodal. For example, if your posterior has Twin Peaks then ordinary Metropolis-Hastings might not be enough. Parallel tempering tends to mitigate these problems and requires nothing more than some extra heated chains.

This vignette demonstrates how parallel tempering can be implemented within *drjacoby* to solve a deliberately awkward MCMC problem.


## Setup

For this example we will start by writing the likelihood and prior functions in C++. If this sounds unfamiliar to you, check out the [earlier vignette](https://mrc-ide.github.io/drjacoby/articles/example.html) for a simple example. Our basic model will assume that our data are normally distributed with mean `alpha^2*beta`. The `alpha^2` term here means that both positive and negative values of `alpha` map to the same value, thereby creating a multimodal distribution, and the `*beta` term ensures that `alpha` and `beta` are highly correlated. We will also use a third parameter `epsilon` to represent some random noise that we want to integrate over. While this is a highly contrived example, it does have the advantage of being horrendously awkward!

For our prior we assume that `alpha` is uniform [-10,10], `beta` is uniform [0,10], and `epsilon` is normally distributed with mean 0 and standard deviation 1.

```{r, echo = FALSE, comment = '', warning=FALSE}
cpp_funcs <- system.file("extdata/", "rungs_cpp_functions.cpp", package = 'drjacoby', mustWork = TRUE)
cpp11::cpp_source(cpp_funcs)
cat(readLines(cpp_funcs), sep = '\n')
```

As always, we need to define a dataframe of parameters so that *drjacoby* knows what parameter ranges to expect:

```{r}
# define parameters dataframe
df_params <- data.frame() |>
  add_parameter(name = "alpha", min = -10, max = 10, initial_values = 5) |>
  add_parameter(name = "beta", min = 0, max = 10, initial_values = 5) |>
  add_parameter(name = "epsilon", min = -Inf, max = Inf, initial_values = 0)
```

Finally, we need some data. For simplicity we will use a series of values drawn from a normal distribution with mean 10. These are obviously not drawn from the true model, but will have the advantage of creating a horribly awkward posterior.

```{r}
# draw example data
data_list <- list(x = rnorm(100, mean = 10))
```


## Running the MCMC

First, we will try running the basic MCMC without parallel tempering. The following block of code repeats the same MCMC analysis nine times, each time producing a plot of posterior `alpha` against `beta`:

```{r, fig.width=8, fig.height=7}
plot_list <- list()
for (i in seq_len(9)) {
  # run MCMC
  mcmc <- dj$new(
    data = data_list,
    df_params = df_params,
    loglike = rung_loglike_cpp11,
    logprior = rung_logprior_cpp11,
    chains = 1
  )
  mcmc$burn(iterations = 1000, silent = TRUE)
  mcmc$sample(iterations = 10000, silent = TRUE)
  
  # create plot of alpha against beta
  plot_list[[i]] <- mcmc$plot_cor("alpha", "beta") +
    ggplot2::ylim(0, 10) + ggplot2::xlim(-10, 10) + ggplot2::theme(legend.position = "none")
}

# plot grid
gridExtra::grid.arrange(grobs = plot_list)
```

Clearly this MCMC is not mixing well! By looking over all nine plots we can get a rough idea of what the distribution *should* look like, but no single MCMC run has captured it adequately. You can experiment with increasing the number of samples - you should get better results for more samples, but a *very* large number of samples are needed before we get good mixing between the left and right sides of the distribution.

Parallel tempering is perfectly designed to solve this kind of problem. To activate parallel tempering, before running the burn-in or sampling we need to run a tuning phse (`$tune()`) to select the number and temperature for our rungs. Each rung represents a different MCMC chain arranged in a "temperature ladder", with the hotter chains being more free to  explore the parameter space. Every iteration, chains have the option of swapping parameter values between rungs, thereby allowing the freely-moving hot chains to propose values to the more constrained colder chains. When looking at the output, we tend to focus exclusively on the coldest chain which can be interpreted the same way as a single chain from regular MCMC.

In this example we use 20 rungs:

```{r, fig.width=5, fig.height=4}
# run MCMC
mcmc <- dj$new(
  data = data_list,
  df_params = df_params,
  loglike = rung_loglike_cpp11,
  logprior = rung_logprior_cpp11,
  chains = 1
)
mcmc$tune(target_rung_acceptance = 0.5,iterations = 5000, silent = TRUE)
mcmc$burn(iterations = 5000, silent = TRUE)
mcmc$sample(iterations = 5000, silent = TRUE)

# create plot of alpha against beta
mcmc$plot_cor("alpha", "beta") +
  ggplot2::ylim(0, 10) + ggplot2::xlim(-10, 10) + ggplot2::theme(legend.position = "none")
```

You should see a much better characterisation of the posterior distribution. The run time scales approximately linearly with the number of rungs, so there is a computational cost to using this method, but on the other hand our results are far better than we would obtain by simply increasing the number of samples by a factor of 20.


## How many rungs to use?

Parallel tempering in *drjacoby* works by proposing swaps between adjacent rungs in the temperature ladder, which are accepted or rejected according to the standard Metropolis-Hastings ratio. If, for example, a hot rung happens upon a set of parameter values that have high likelihood then there is a good chance these values will be swapped up to the next rung. In this way, information can effectively move its way from the prior (the hottest chain) to the posterior (the coldest chain). The average chance of a swap being accepted depends on the two adjacent rungs being similar enough in distribution that values drawn from one have a high likelihood in the other.

The `$tune()` function works by selecting the number and temperature of rungs against a target rate. It first runs the mcmc for a short period with 
a resonably high number of rungs, the default being 50. We can use the output from this phase to inform us how hard it is to traverse from the coldest
rung to the hottest - we call this the local communication barrier. We can view the local communication barrier across all rungs:

```{r, fig.width=5, fig.height=4}
mcmc$plot_local_communication_barrier()
```

You can read this just like an elevation plot for a hike! Higher peaks are harder to traverse and indicate we may need to focus more rungs in that area
so that they can communicate with each other. Using this, drjacoby automatically picks the number and temperatures of the rungs for the burn-in and sampling phases. We can view the acceptance rates of these rungs to check they are close to the target rung acceptance rate specified in the tuning phase (in this case it was 0.5):

```{r, fig.width=5, fig.height=4}
mcmc$plot_mc_acceptance_rate()
```

We can see that the acceptance rates are mostly ~0.5 all the way through our temperature ladder, and all >0 so we can be confident that the prior is "talking to" the posterior, making it *extremely unlikely* (but not impossible) that the MCMC is still missing large parts of the posterior distribution.
